{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-06T16:32:02.276102Z",
     "start_time": "2024-05-06T16:32:02.234824Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.generativeai' has no attribute 'GenerativeModel'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mast\u001B[39;00m\n\u001B[0;32m      6\u001B[0m genai\u001B[38;5;241m.\u001B[39mconfigure(api_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAIzaSyDw_fNpdpLQbRtDbBSPyvzsbUg4Oh0De7k\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mgenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mGenerativeModel\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgemini-pro\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      9\u001B[0m response \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate_content(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWrite a story about a magic backpack.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(response\u001B[38;5;241m.\u001B[39mtext)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'google.generativeai' has no attribute 'GenerativeModel'"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "import json\n",
    "import ast\n",
    "\n",
    "genai.configure(api_key='')\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "response = model.generate_content(\"Write a story about a magic backpack.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.generativeai' has no attribute 'GenerativeModel'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 36\u001B[0m\n\u001B[0;32m     10\u001B[0m generation_config \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     11\u001B[0m   \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m     12\u001B[0m   \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0.95\u001B[39m,\n\u001B[0;32m     13\u001B[0m   \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_k\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m     14\u001B[0m   \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_output_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m8192\u001B[39m,\n\u001B[0;32m     15\u001B[0m }\n\u001B[0;32m     17\u001B[0m safety_settings \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     18\u001B[0m   {\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHARM_CATEGORY_HARASSMENT\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     33\u001B[0m   },\n\u001B[0;32m     34\u001B[0m ]\n\u001B[1;32m---> 36\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mgenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mGenerativeModel\u001B[49m(model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgemini-1.5-pro-latest\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     37\u001B[0m                               generation_config\u001B[38;5;241m=\u001B[39mgeneration_config,\n\u001B[0;32m     38\u001B[0m                               safety_settings\u001B[38;5;241m=\u001B[39msafety_settings)\n\u001B[0;32m     40\u001B[0m convo \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mstart_chat(history\u001B[38;5;241m=\u001B[39m[\n\u001B[0;32m     41\u001B[0m ])\n\u001B[0;32m     43\u001B[0m convo\u001B[38;5;241m.\u001B[39msend_message(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWrite a story about a magic backpack.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'google.generativeai' has no attribute 'GenerativeModel'"
     ]
    }
   ],
   "source": [
    "# read training files\n",
    "prompts = []\n",
    "with open('training_data/train_1m', 'r', encoding='utf-8') as f:\n",
    "    content = f.readlines()\n",
    "    prompt = \"\"\n",
    "    for line in content:\n",
    "        if line == '<end>\\n':\n",
    "            prompts.append(prompt)\n",
    "            prompt = \"\"\n",
    "            continue\n",
    "        prompt += line\n",
    "\n",
    "SUF = ['', '_CoT', '_ToT']\n",
    "_suf = SUF[2]\n",
    "\n",
    "tot_prompt = \"Imagine five different experts answering this question. All experts will write down 1 step of their thinking and then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they're wrong at any point, then they leave. And show me the result in the end. \\nThe question is:\\n\\nYou are a recommender system. Given the user profile and historical data, analyze the user's interests and try to recommend movies the user might like. Your task is to add at least five movies between the historical data and the target movie to connect them as an influence path. Then recommend them to the user one by one. Any adjacent movies should have a strong relation with each other, and make sure the movies are not included in the historical data. The movies should be before 2001.\\n\"\n",
    "\n",
    "cot_prompt = \"You are a recommender system. Given the user profile and historical data, analyze the user's interests and try to recommend movies the user might like. Your task is to add at least five movies between the historical data and the target movie to connect them as an influence path. Then recommend them to the user one by one. Any adjacent movies should have a strong relation with each other, and make sure the movies are not included in the historical data. Think step by step and make sure. The movies should be before 2001.\\n\"\n",
    "\n",
    "plain_prompt = \"You are a recommender system. Given the user profile and historical data, analyze the user's interests and try to recommend movies the user might like. Your task is to add at least five movies between the historical data and the target movie to connect them as an influence path. Then recommend them to the user one by one. Any adjacent movies should have a strong relation with each other, and make sure the movies are not included in the historical data. The movies should be before 2001.\\n\"\n",
    "\n",
    "j = 0\n",
    "with open(f\"training_data/output_gpt_1m{_suf}_pro\", \"w\", encoding='utf-8') as f:\n",
    "    with open(f\"training_data/output_gpt_1m{_suf}_pro.json\", \"w\", encoding='utf-8') as fj:\n",
    "        result = []\n",
    "\n",
    "        for prompt in prompts:\n",
    "            j += 1\n",
    "            print(j)\n",
    "            for i in range(2):\n",
    "                message = [\n",
    "                    {\"role\": \"system\", \"content\": cot_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=message\n",
    "                )\n",
    "\n",
    "                message.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
    "                message.append({\"role\": \"user\",\n",
    "                                \"content\": \"Output the influence path in the format of python list object. Example output:\\n[movie name1,\\nmovie name2,\\nmovie name3,\\nmovie name4,\\n...\\ntarget movie name]\\n\"})\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=message\n",
    "                )\n",
    "\n",
    "                f.write(prompt)\n",
    "                f.write(\"\\nInfluence path:\\n\")\n",
    "                influence_path = completion.choices[0].message.content\n",
    "\n",
    "                try:\n",
    "                    influence_path_get = influence_path[influence_path.index(\"[\"):influence_path.index(\"]\") + 1]\n",
    "                    influence_path_lst = ast.literal_eval(influence_path_get)\n",
    "                except:\n",
    "                    print(f\"GPT输出不合规:{influence_path}\")\n",
    "                    influence_path_lst = []\n",
    "\n",
    "                for influence in influence_path_lst:\n",
    "                    f.write(f\"{influence}\\n\")\n",
    "                f.write(\"<end>\\n\")\n",
    "\n",
    "                if i == 0:\n",
    "                    result.append([influence_path_lst])\n",
    "                elif i == 1:\n",
    "                    result[-1].append(influence_path_lst)\n",
    "        json.dump(result, fj)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:06:52.076078Z",
     "start_time": "2024-05-06T14:06:52.058758Z"
    }
   },
   "id": "6c3b68d44a4b6ba1",
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
